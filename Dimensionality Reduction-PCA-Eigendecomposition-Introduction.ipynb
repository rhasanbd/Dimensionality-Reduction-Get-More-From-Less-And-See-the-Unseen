{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Principle Component Analysis (PCA) via Eigendecomposition\n",
    "\n",
    "Principal Component Analysis (PCA) is a linear method for dimensionality reduction. Using PCA we can project the data matrix onto a de-correlated set of bases. We take the $d$-dimensional **data covariance matrix** and diagonalize it such that the new bases are de-correlated. Then, we project the data on a subset of the news axes that has high variance (covers most of the variance of the data).\n",
    "\n",
    "The diagonalization of the data covariance matrix in PCA can be done by performing its **eigendecomposition**. The eigendecomposition is a matrix decomposition technique for reducing a matrix into its constituent parts. More specifically, eigendecomposition decomposes a matrix into eigenvectors and eigenvalues. \n",
    "\n",
    "In eigendecomposition we solve the eigen equation of the data covariance matrix. The eigenvectors represent the new bases that are de-correlated. These eigenvectors are known as the priciple axes or componnents of the data. The eigenvalues represent the variance of each principle component. By using the eigenvectors corresponding to the largest $k$ eigenvalues we can project the $d$-dimensional data onto a lower $k$ dimesional space.\n",
    "\n",
    "In this notebook, we use a toy data matrix to implement eigendecomposition based PCA using two approaches.\n",
    "- Manual Implementation\n",
    "- Scikit-Learn based Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Data Matrix X from Sample Vectors\n",
    "\n",
    "Consider that we have 3 samples each with two features:\n",
    "- $\\vec{x}_1 = [1, 2]$\n",
    "- $\\vec{x}_2 = [3, 4]$\n",
    "- $\\vec{x}_3 = [5, 6]$\n",
    "\n",
    "Here, $N = 3$ and $d = 2$\n",
    "\n",
    "Using these 3 samples we can create the data matrix X as follows:\n",
    "- Use 3 samples as 3 row vectors and horizontally stack them.\n",
    "        \n",
    "        Dimension of X: N x d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Matrix X:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "X Dimension (N x d):  (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# 3 Data Samples\n",
    "x_1 = [1, 2]\n",
    "x_2 = [3, 4]\n",
    "x_3 = [5, 6]\n",
    "\n",
    "\n",
    "print(\"\\nData Matrix X:\")\n",
    "X = np.array([x_1, x_2, x_3])\n",
    "print(X)\n",
    "print(\"\\nX Dimension (N x d): \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Implementation of Eigendecomposition based PCA\n",
    "\n",
    "There are two Steps\n",
    "- Center the Data and Compute Its Covariance Matrix\n",
    "- Eigendecomposition: Solve the eigen equation of the covariance matrix\n",
    "\n",
    "### Center the Data and Compute Its Covariance Matrix \n",
    "\n",
    "First, we center the data by subtracting the mean of each feature (column).\n",
    "\n",
    "Then, we compute the covariance matrix as follows. Note that in the denominator we used $N-1$, instead on $N$, for an unbiased estimator.\n",
    "\n",
    "$ cov(X_{centered}) = \\frac{X^T_{centered}.(X_{centered})}{N - 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "X dimension (N x d):  (3, 2)\n",
      "\n",
      "Mean of X:\n",
      " [3. 4.]\n",
      "\n",
      "X Centered:\n",
      " [[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "\n",
      "Covariance Matrix:\n",
      " [[4. 4.]\n",
      " [4. 4.]]\n",
      "\n",
      "Covariance Matrix (using NumPy):\n",
      " [[4. 4.]\n",
      " [4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"X:\\n\", X)\n",
    "print(\"\\nX dimension (N x d): \", X.shape)\n",
    "\n",
    "\n",
    "# Compute the mean of the data (mean of each feature/column)\n",
    "X_mean = np.mean(X, axis=0)\n",
    "print(\"\\nMean of X:\\n\", X_mean )\n",
    "\n",
    "\n",
    "# Center the data (subtract the mean)\n",
    "X_centered = X - X_mean\n",
    "print(\"\\nX Centered:\\n\", X_centered)\n",
    "\n",
    "\n",
    "# Compute the covariance matrix of the data\n",
    "cov_X =  X_centered.T.dot(X_centered)/(X.shape[0]-1)\n",
    "print(\"\\nCovariance Matrix:\\n\", cov_X)\n",
    "\n",
    "# Compute the covariance matrix of the data using NumPy function\n",
    "\n",
    "\n",
    "print(\"\\nCovariance Matrix (using NumPy):\\n\", np.cov(X_centered.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "We find the eigenvectors and eigenvalues of the data covariance matrix (centered). The eigevectors are the new bases that are de-correlated. These are known as **Principle Components (PCs)**. After projecting the data matrix on the PCs, the dimension of the projected data matrix should be: $N x d$ \n",
    "\n",
    "Note that the dimension of X and eigenvector matrix (N = no. of data & d = no. of features) should be:\n",
    "\n",
    "- X: N x d \n",
    "\n",
    "- Eigenvector: d x d\n",
    "\n",
    "\n",
    "If we take a subset of the PCs ($d_{pc} \\leq d$), then dimension of the projected data matrix should be: $N x d_{pc}$\n",
    "\n",
    "Thus, to compute the projected data matrix, we need to take the dot product between \"X\" and \"eigenvector\".\n",
    "\n",
    "Finally, after reconstructing \"X\", the dimension of \"X_reconstructed\" should be $N x d$\n",
    "\n",
    "This can be ensured by taking the dot product between projected data matrix ($N x d_{pc}$) and the transpose of the eigenvector ($d_{pc} x d$).\n",
    "\n",
    "Dimension of X_reconstructed: $(N x d_{pc}) x (d_{pc} x d) = N x d$\n",
    "\n",
    "Below we show two extanples of performing eigendecomposition.\n",
    "\n",
    "- Example 1 (No reduction in dimension): Project the Data using all PCs\n",
    "- Example 2 (Dimensionality dimension): Project the Data using a subset of the PCs (e.g., the PC with the largest variance/eigenvalue)\n",
    "\n",
    "\n",
    "## Example 1 (No reduction in dimension): Project the Data using all PCs\n",
    "\n",
    "Using two PCs we project the 2D data onto 2D. Thus, there is no dimensionality reduction. As a consequence we should be able to reconstruct the original data matrix (centered). We will see that, reconstrction error would be vanishingly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eigenvectors (Principle Components):\n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "\n",
      "Eigenvalues (Variances):\n",
      " [8. 0.]\n",
      "\n",
      "***Observe that the first PC explains most of the variance (largest eigenvalue) in the data.***\n",
      "\n",
      "X dimension:  (3, 2)\n",
      "Covariance matrix dimension:  (2, 2)\n",
      "Eigenvalues dimension:  (2,)\n",
      "Eigenvectors dimension:  (2, 2)\n",
      "\n",
      "Projected Data Matrix (all PC):\n",
      " [[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n",
      "\n",
      "X centered (reconstructed):\n",
      " [[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "\n",
      "X Centered (original):\n",
      " [[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "\n",
      "Overall Reconstruction Error (all PC):  1.314768175368353e-31\n"
     ]
    }
   ],
   "source": [
    "# Eigendecomposition of covariance matrix: find eigenvectors & eigenvalues of cov_X\n",
    "eigenvalues, eigenvectors = eig(cov_X)\n",
    "\n",
    "print(\"\\nEigenvectors (Principle Components):\\n\", eigenvectors)\n",
    "print(\"\\nEigenvalues (Variances):\\n\", eigenvalues)\n",
    "\n",
    "\n",
    "print(\"\\n***Observe that the first PC explains most of the variance (largest eigenvalue) in the data.***\")\n",
    "\n",
    "print(\"\\nX dimension: \", X.shape)\n",
    "print(\"Covariance matrix dimension: \", cov_X.shape)\n",
    "print(\"Eigenvalues dimension: \", eigenvalues.shape)\n",
    "print(\"Eigenvectors dimension: \", eigenvectors.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Project the data on ALL eigenvectors, i.e., Principle components (PCs)\n",
    "X_projected_all_pc = X_centered.dot(eigenvectors)\n",
    "print(\"\\nProjected Data Matrix (all PC):\\n\", X_projected_all_pc)\n",
    "\n",
    "\n",
    "\n",
    "# Reconstruct the original Data Matrix (centered)\n",
    "X_reconstructed_all_pc = X_projected_all_pc.dot(eigenvectors.T)\n",
    "print(\"\\nX centered (reconstructed):\\n\", X_reconstructed_all_pc)\n",
    "\n",
    "\n",
    "print(\"\\nX Centered (original):\\n\", X_centered)\n",
    "\n",
    "\n",
    "# Compute overall reconstruction error using the centered data\n",
    "reconstruction_error_all_pc = mean_squared_error(X_centered, X_reconstructed_all_pc)\n",
    "print(\"\\nOverall Reconstruction Error (all PC): \", reconstruction_error_all_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 (Dimensionality dimension): Project the Data using a subset of the PCs (e.g., the PC with the largest variance/eigenvalue)\n",
    "\n",
    "The first eigenvector (PC) has the largets eigenvalue (variance). Thus, using the first PC we project the 2D data onto 1D. This will reduce the dimension of data without losing much variance. As a result, we will see that the reconstrction error would be vanishingly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Principle Component:\n",
      " (2, 1)\n",
      "\n",
      "Projected Data Matrix (first PC):\n",
      " [[-2.82842712]\n",
      " [ 0.        ]\n",
      " [ 2.82842712]]\n",
      "\n",
      "X centered (reconstructed):\n",
      " [[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "\n",
      "X Centered (original):\n",
      " [[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "\n",
      "Overall Reconstruction Error (first PC):  1.314768175368353e-31\n",
      "\n",
      "Increase in reconstruction error due to dimensionality reduction:  0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Take the first eigenvector (PC) that has the largets eigenvalue (variance)\n",
    "PC_first = eigenvectors[:,0].reshape(2, 1)\n",
    "print(\"\\nFirst Principle Component:\\n\", PC_first.shape)\n",
    "\n",
    "\n",
    "# Project the data on the largest eigenvector, i.e., first PC\n",
    "X_projected_first_pc = X_centered.dot(PC_first)\n",
    "print(\"\\nProjected Data Matrix (first PC):\\n\", X_projected_first_pc)\n",
    "\n",
    "\n",
    "\n",
    "# Reconstruct the original Data Matrix (centered)\n",
    "X_reconstructed_first_pc = X_projected_first_pc.dot(PC_first.T)\n",
    "print(\"\\nX centered (reconstructed):\\n\", X_reconstructed_first_pc)\n",
    "\n",
    "\n",
    "print(\"\\nX Centered (original):\\n\", X_centered)\n",
    "\n",
    "\n",
    "# Compute overall reconstruction error using the centered data\n",
    "reconstruction_error_first_pc = mean_squared_error(X_centered, X_reconstructed_first_pc)\n",
    "print(\"\\nOverall Reconstruction Error (first PC): \", reconstruction_error_first_pc)\n",
    "\n",
    "\n",
    "print(\"\\nIncrease in reconstruction error due to dimensionality reduction: \", (reconstruction_error_first_pc - reconstruction_error_all_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn based Implementation of PCA\n",
    "\n",
    "We can perform PCA by using Scikit-Learn's PCA class.\n",
    "\n",
    "We set the \"n_components\" parameter. It represents the number of components to keep. If n_components is not set all components are kept.\n",
    "\n",
    "Following notebook provides a description of other parameters of the Scikit-Learn PCA:\n",
    "https://github.com/rhasanbd/Dimensionality-Reduction-Get-More-From-Less-And-See-the-Unseen/blob/master/Dimensionality%20Reduction-1-Linear%20Methods.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Principle Components (Eigenvectors ):  2\n",
      "\n",
      "Pinciple Components:\n",
      " [[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n",
      "\n",
      "Explained Variance (eigenvalues) by each PC:  [8.00000000e+00 2.25080839e-33]\n",
      "\n",
      "Explained variance ratio: [1.00000000e+00 2.81351049e-34]\n",
      "\n",
      "***Observe that the first PC explains most of the variance in the data.***\n",
      "\n",
      "Projected Data Matrix:\n",
      " [[-2.82842712e+00  2.22044605e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82842712e+00 -2.22044605e-16]]\n",
      "\n",
      "X Reconstructed:\n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "\n",
      "X (original):\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "Overall Reconstruction Error (using centered X):  8.217301096052206e-33\n"
     ]
    }
   ],
   "source": [
    "# Create a PCA object\n",
    "pca = PCA(n_components=2)\n",
    "# Fit the object on the data matrix X\n",
    "pca.fit(X)\n",
    "\n",
    "\n",
    "print(\"\\nNumber of Principle Components (Eigenvectors ): \", pca.n_components_)  \n",
    "\n",
    "print(\"\\nPinciple Components:\\n\", pca.components_)\n",
    "print(\"\\nExplained Variance (eigenvalues) by each PC: \", pca.explained_variance_)\n",
    "\n",
    "# Percentage of variance explained for each component\n",
    "print(\"\\nExplained variance ratio: %s\"\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "print(\"\\n***Observe that the first PC explains most of the variance in the data.***\")\n",
    "\n",
    "\n",
    "# Find the projected data matrix using the transform method\n",
    "X_projected_sklearn = pca.transform(X)\n",
    "print(\"\\nProjected Data Matrix:\\n\", X_projected_sklearn)\n",
    "\n",
    "\n",
    "# Reconstruct the original data matrix\n",
    "X_reconstructed_sklearn = pca.inverse_transform(X_projected_sklearn)\n",
    "print(\"\\nX Reconstructed:\\n\", X_reconstructed_sklearn)\n",
    "\n",
    "\n",
    "print(\"\\nX (original):\\n\", X)\n",
    "\n",
    "\n",
    "# Compute overall reconstruction error using the centered data\n",
    "reconstruction_error = mean_squared_error(X, X_reconstructed_sklearn)\n",
    "print(\"\\nOverall Reconstruction Error (using centered X): \", reconstruction_error)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
